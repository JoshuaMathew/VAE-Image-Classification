# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mF5VzJKIPtZKMkDwZevXiK-zbzOxI2X
"""

pip install import-ipynb

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import sys
sys.path.append('/content/gdrive/Shared drives/ECE765 Group Project')
# %cd /content/gdrive/Shared drives/ECE765 Group Project

import numpy as np
import matplotlib.pyplot as plt
import import_ipynb
import time
import h5py
import joblib
from sklearn.decomposition import PCA
from torch.utils.data import Dataset, DataLoader
import DataPrep_CNN
import torch

#Build Training Dataset

class TrainingDataset(Dataset):
    def __init__(self):
      self.preimages = read_training_data()
      self.images = normalized_images(self.preimages)
      self.labels = np.array([0] * 4739 + [1] * 4738)

    def __len__(self):
      return len(self.images)

    def __getitem__(self, idx):
      img = self.images[idx]
      label = self.labels[idx]
      return img, label

def trainDataset():
  train_dataset = TrainingDataset()
  return train_dataset

#Build Validation Dataset

class ValDataset(Dataset):
    def __init__(self):
      self.preimages = read_val_data()
      self.images = normalized_images(self.preimages)
      self.labels = np.array([0] * 500 + [1] * 500)

    def __len__(self):
      return len(self.images)

    def __getitem__(self, idx):
      img = self.images[idx]
      label = self.labels[idx]
      return img, label

def valDataset():
  val_dataset = ValDataset()
  return val_dataset

# load training and validation sets
trainImages = torch.load('/content/gdrive/Shared drives/ECE765 Group Project/trainImages_vae.pt')
valImages = torch.load('/content/gdrive/Shared drives/ECE765 Group Project/valImages_vae.pt')
x = trainImages.images.numpy()
x_dataset = x[0:4738]
x_dataset = np.moveaxis(x_dataset,1,3)
x_dataset.shape

def train_pca(X_dataset, n_components, N=4738):
  
    np.random.seed(0)
    index = np.random.choice(len(X_dataset), N, replace=False)
    X_dataset = X_dataset[index]
    
    print('Sample Size: %d' % N)
    print('Data Type X=' + str(X_dataset.dtype))
    print('Shape X=' + str(X_dataset.shape))
    
    pca = PCA(n_components=n_components, whiten=False)
    
    start = time.time()
   
    # Reshape to 1-D array
    shape = X_dataset.shape
    X_reshape = X_dataset.reshape((shape[0], -1)) / 255.
    X_reshape.shape
   
    pca.fit(X_reshape)
    X_pca = pca.transform(X_reshape)
    
    end = time.time()
    elapsed = end - start
    print('Fit time elapsed: {}'.format(elapsed))
    
    return pca
   
n_components = 2000
pca = train_pca(x_dataset, n_components=n_components)

model_path = '/content/gdrive/Shared drives/ECE765 Group Project/pca128_2000comp' 
print('Dumping pca  model to: %s' % model_path) 
joblib.dump(pca, model_path)

model_path = '/content/gdrive/Shared drives/ECE765 Group Project/pca128_1000comp'
pca = joblib.load(model_path)

model_path2 = '/content/gdrive/Shared drives/ECE765 Group Project/pca128_200comp'
pca200 = joblib.load(model_path2)

def display_grid(dataset, digit_size=128, grid_size=3, seed=None):
    # Display some digits to figure out what's going on
    figure = np.zeros((digit_size * grid_size, digit_size * grid_size, 3))
   
    if seed is not None:
        np.random.seed(seed)
    for i in range(grid_size):
        for j in range(grid_size):
            digit = dataset[np.random.randint(len(dataset))]
            d_x, d_y = i * digit_size, j * digit_size
            figure[d_x:d_x + digit_size, d_y:d_y + digit_size, :] = digit.astype(int)
            
    plt.figure(figsize=(7, 7))
    plt.imshow(figure)
    plt.show()

# display random set of images and their PCA reconstructions
sample_size = 1000
print("Originals")
index = np.random.choice(len(x_dataset), sample_size, replace=False)
dataset = x_dataset[index]
display_grid(dataset, seed=5)

print("Reconstructed PCA")
X_reshape = dataset.reshape((dataset.shape[0], -1)) / 255.
X_pca = pca.transform(X_reshape)
X_recon = np.clip(pca.inverse_transform(X_pca), 0.0, 0.999)
X_recon_reshape = X_recon.reshape(dataset.shape) * 255.
display_grid(X_recon_reshape, seed=5)

plt.imshow(x_dataset[index[0]])

#image = trainImages.images[29].numpy()
image = valImages.images[0].numpy()
image = np.moveaxis(image,0,2)
image = image
#image = np.moveaxis(image,0,2)
#image = image.reshape(1,128,128,3)
fig = plt.figure(figsize=(10,10))
ax1 = fig.add_subplot(231)
ax1.title.set_text('Original')
plt.imshow(np.squeeze(image))

ax2 = fig.add_subplot(232)
ax2.title.set_text('PCA Reconstruction')
X_sample = np.stack([image] * 1000)
X_reshape = image.flatten().reshape(1, -1) / 255.
X_pca = pca.transform(X_reshape)
X_recon = np.clip(pca.inverse_transform(X_pca), 0.0, 0.999)
X_recon_reshape = X_recon.reshape(image.shape) * 255.
plt.imshow(np.squeeze(X_recon_reshape))

ax2 = fig.add_subplot(233)
ax2.title.set_text('VAE Reconstruction')
X_sample = np.stack([image] * 1000)
X_reshape = image.flatten().reshape(1, -1) / 255.
X_pca200 = pca200.transform(X_reshape)
X_recon200 = np.clip(pca200.inverse_transform(X_pca200), 0.0, 0.999)
X_recon_reshape200 = X_recon200.reshape(image.shape) * 255.
plt.imshow(np.squeeze(X_recon_reshape200))
